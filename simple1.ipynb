{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 61878 rows and 95 columns\n",
      "   feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
      "0       1       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       1       0   \n",
      "2       0       0       0       0       0       0       0       1       0   \n",
      "3       1       0       0       1       6       1       5       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   feat_10   ...     feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  \\\n",
      "0        0   ...           0        1        0        0        0        0   \n",
      "1        0   ...           0        0        0        0        0        0   \n",
      "2        0   ...           0        0        0        0        0        0   \n",
      "3        1   ...          22        0        1        2        0        0   \n",
      "4        0   ...           0        1        0        0        0        0   \n",
      "\n",
      "   feat_90  feat_91  feat_92  feat_93  \n",
      "0        0        0        0        0  \n",
      "1        0        0        0        0  \n",
      "2        0        0        0        0  \n",
      "3        0        0        0        0  \n",
      "4        1        0        0        0  \n",
      "\n",
      "[5 rows x 93 columns]\n",
      "10\n",
      "RFC LogLoss 0.6851102151214558\n",
      "LogisticRegression LogLoss 0.6724898822712051\n",
      "RFC2 LogLoss 0.6541558264501615\n",
      "Ensamble Score: 0.5636216904851277\n",
      "Best Weights: [ 0.42491759  0.13610349  0.43897892]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import os\n",
    "\n",
    "# os.system(\"ls ../input\")\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "print(\"Training set has {0[0]} rows and {0[1]} columns\".format(train.shape))\n",
    "\n",
    "labels = train['target']\n",
    "train.drop(['target', 'id'], axis=1, inplace=True)\n",
    "\n",
    "print(train.head())\n",
    "\n",
    "### we need a test set that we didn't train on to find the best weights for combining the classifiers\n",
    "sss = StratifiedShuffleSplit(labels, test_size=0.05, random_state=1234)\n",
    "print(len(sss))\n",
    "for train_index, test_index in sss:\n",
    "    break\n",
    "\n",
    "train_x, train_y = train.values[train_index], labels.values[train_index]\n",
    "test_x, test_y = train.values[test_index], labels.values[test_index]\n",
    "\n",
    "### building the classifiers\n",
    "clfs = []\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=4141, n_jobs=-1)\n",
    "rfc.fit(train_x, train_y)\n",
    "print('RFC LogLoss {score}'.format(score=log_loss(test_y, rfc.predict_proba(test_x))))\n",
    "clfs.append(rfc)\n",
    "# print(clfs)\n",
    "\n",
    "### usually you'd use xgboost and neural nets here\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_x, train_y)\n",
    "print('LogisticRegression LogLoss {score}'.format(score=log_loss(test_y, logreg.predict_proba(test_x))))\n",
    "clfs.append(logreg)\n",
    "# print(clfs)\n",
    "\n",
    "rfc2 = RandomForestClassifier(n_estimators=50, random_state=1337, n_jobs=-1)\n",
    "rfc2.fit(train_x, train_y)\n",
    "print('RFC2 LogLoss {score}'.format(score=log_loss(test_y, rfc2.predict_proba(test_x))))\n",
    "clfs.append(rfc2)\n",
    "# print(clfs)\n",
    "\n",
    "\n",
    "### finding the optimum weights\n",
    "\n",
    "predictions = []\n",
    "for clf in clfs:\n",
    "#     print(\"hello\")\n",
    "    predictions.append(clf.predict_proba(test_x))\n",
    "#     print(predictions)\n",
    "\n",
    "def log_loss_func(weights):\n",
    "    ''' scipy minimize will pass the weights as a numpy array '''\n",
    "    final_prediction = 0\n",
    "    for weight, prediction in zip(weights, predictions):\n",
    "            final_prediction += weight*prediction\n",
    "\n",
    "    return log_loss(test_y, final_prediction)\n",
    "    \n",
    "#the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "#its better to choose many random starting points and run minimize a few times\n",
    "starting_values = [0.5]*len(predictions)\n",
    "\n",
    "#adding constraints  and a different solver as suggested by user 16universe\n",
    "#https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "#our weights are bound between 0 and 1\n",
    "bounds = [(0,1)]*len(predictions)\n",
    "\n",
    "res = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "\n",
    "print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n",
    "print('Best Weights: {weights}'.format(weights=res['x']))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
